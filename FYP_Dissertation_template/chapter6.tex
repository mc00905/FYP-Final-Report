\chapter{Testing and Evaluation}
\section{Introduction}

This section will detail my plans on how I will test the framework that I have put together and how I will evaluate how well it meets the objectives and specifications that I have defined for it. 

Creating a test suite for micro-services typically involves a lot of different moving parts, and I will begin by explaining how I collected a number of libraries and tools to create a comprehensive set of unit tests. I will showcase these tools by creating an exemplar test suite for the shopping micro-service, with all of the design and architectural decisions made during the development of the test suite being explained. Whilst the test suite will be written for the exemplar micro-service developed for this project, the methodology for the design and implementation of the unit tests will be applicable for any micro-service built using the template that I have created.


Each of the software components need to be evaluated against the original objectives and specifications lined out for them. The second half of the chapter will contain this evaluation with justification for whether I feel that the individual objectives have been met.


\section{Unit Testing with SuperTest, Jest and Nock}

My Unit Tests for both the micro-service and the client will use a number of key libraries, SuperTest, Jest and Nock. Whilst they will test the sample services I have created, the main aim will be to produce a suite of tools for general micro-service testing.
\subsection{SupertTest}
SuperTest is a Node.js library used for testing HTTP. By passing in the server export from the App.ts we can perform Unit Tests on the actual HTTP requests made through the server.

All aspects of the response can be tested including headers and status code, with the body of the response being the focal point of my testing. This will allow me to ensure that everything returned by the API is returned in both the format and shape that is expected, and the appropriate errors are thrown on respective requests.
\subsection{Jest}
Jest is a lightweight and very user-friendly JavaScript testing framework developed by Facebook. The tests themselves are split into separate processes and run in parallel, resulting in extremely fast and performant test suites.

Jest requires very little boilerplate configuration, and pretty much works out of the box making it very easy to get straight to writing Unit Tests on the code-base.

The framework also has the Istanbul code coverage tool built in. Istanbul tracks how much of our code is actually covered by our Unit Tests such as:
\begin{itemize}
    \item Functions: Are all the functions and methods tested
    \item Branching: Are all branches in the decision-making logic tested
    \item Statements: Are all variable assignments, return values etc tested
\end{itemize}

The reports generated by Istanbul detail what code isn't covered too, allowing the test suite to be expanded to cover these areas. The high the code coverage, the more of the code-base that is tested, which in turn should reduce the numbers of bugs introduced to the system.

It should be noted that Code Coverage and Test Coverage are different. Where Code Coverage tests how much of the code is tested in some way, Test Coverage measures those tests against actual Specifications and Requirements, testing the actual coverage of the expected functionality. As such where Code Coverage shines for Unit Tests, Test Coverage is a very good metric for Acceptance Testing.

For this project I will only be testing Code Coverage as the bulk of my physical tests will be Unit Tests.
\subsection{Nock}
Nock is a Node.js library used for mocking HTTP requests. Nock intercepts the HTTP requests sent by the server or application, and rather than actually forwarding the error to the destination it returns a static response body or error defined by the developer. 

It is particularly useful for interacting with external APIs as it mimics the request sent by the consuming service without actually interacting within the API. This allows the unit tests to be run without connecting to the internet or running any of the dependencies of the service being tested locally. 

Nock allows creation of highly configurable mocks and every component of the requests and responses can be refined. This includes everything from headers to status codes.

Once a request is intercepted the mock is consumed. This allows different instances of the exact same request to return different responses. This is particularly useful for testing things like rate limit handling, as the first few requests which are limited will fail with a 429 status code, but subsequent requests that are not limited will return successfully.

All of the unit tests for my API Client will make use of Nock so that it can be tested in isolation from the micro-service.
\subsection{Code Coverage Target}
Oftentimes it can be extremely challenging to hit one hundred percent code coverage. While I will aim to hit as close to complete coverage as possible, I will be aiming for a minimum of 85\% of overall code coverage for the important files within both the client and micro-service. There are likely to be certain parts of the code that aren't feasible to reach during unit-testing, particularly around handling of unexpected failures around the database connections.

It is important in software projects to identify a suitable balance between effort involved in increasing coverage targets and the benefits gained. Whilst the process of identifying test objectives is out of scope for this project, we have informally used a risk-based testing approach \cite{Amland} to focus tests on those areas of code with highest exposure, and reduced testing effort on low exposure areas of code where the potential impact of faults is low. The important result is that the test tools that have been identified fully support risk-based testing.
\subsection{Unit Testing the Micro-service}
The testing of the micro-service will be broken down into two key sections, testing that the controllers work and respond as expected, and that the data-agents process and store the data correctly.
\subsubsection{Testing the Controllers}
In order to test the controllers and the REST API exposed through them I will make use of the SuperTest library. SuperTest allows programmatic HTTP requests to be made to the server within the unit tests, allowing testing of the same URL endpoints that the live API will expose.

Upon calling the HTTP requests I will use Jest assertions to verify that the fields returned within the response are correct. To do this I will have to create static response bodies for each of the requests that I wish to test and compare them against those actually returned by the request. If the fields match those expected, then the API is working as expected.

Each of the URL endpoints within the micro-service will have a small suite of unit tests written for them. Requests that should trigger successful responses will be validated as will as requests that should trigger expected errors, such as doing GET requests on resources that do not exist. The HTTP status codes will be tested, ensuring that they are being assigned correctly and any response or error bodies will also be checked to ensure that they contain all of the content that they should. Essentially any documented request and response for the API should be tested.
\subsubsection{Testing the Data Agents \& Models}
Testing the data-agents and models involves interacting with the actual database operations. We do not want our unit tests ever interacting with live data and so standard practise is to connect to a completely separate test-focused database. This is easy to do with Mongoose, and we can simple create a new database connection as part of the unit tests. Once the database connection is defined we can simple inject a number of test documents into the collections to act as our test data.

A small sample of test documents will be created upon the start up of the unit tests and saved into the test database. Different values for each of the model fields will be used across the suite of documents. This will allow tests to be written that verify that the correct subset of the collection is returned for the different filtering and sorting options on the data-agent methods.

Each of the operation involved in fetching existing documents will utilise the test documents created upon database startup. This will allow assertions to be run against the actual document definitions reducing boilerplate and keeping the unit tests lightweight. However, for the operations involving modifying or creating documents the tests will consist of two stages. The first will be defining the parameters used for the creation or modification or the resource, as well as creating a JSON representation of the resource after the operation has been defined. The method can then be called assertions can be tested to ensure that the operation did not fail. In order to test that the actual data saved is what is expected, a second request fetching the new data needs to be made. This can be done directly through the mongoose model using the primary key used in the create or modify operation, and the response can be tested against the previously defined JSON object. This two-stage process ensures that not only does the operation not fail, the actual information stored in database is exactly what is expected.
\subsection{Unit Testing the Handcrafted Client}
Testing the client will involve using Nock to intercept and mock the responses of all the outgoing requests that the client sends out. These will simulate both successful and failed requests and allow all expected cases to be tested against.
\subsubsection{Intercepting HTTP Requests to the micro-service with Nock}
There are a number of different behavioural aspects that need to be tested within the client. The successful responses need to be properly handled and parsed into objects that are expected, any errors thrown by the API need to be caught and handled without losing any crucial information and certain failed requests need to be retried automatically.

Testing handling of sucessful responses is easy with Nock. The endpoint simply need to passed into the Nock constructor before calling the \textit{.reply()} method with both the status code and response body that the mocked request should return. An example can be seen below.
\begin{verbatim}
    const request = Nock('http://localhost:3000/REST/1.0')
    .get('/shoppingItems') 
    .reply(200,
        {
            page: 1,
            totalPages: 1,
            shoppingItems: [{
                "name":"apple",
                "category":"Fruit",
                "numberOfStock":110,
                "inStock":true
            }]
        })
    const res = await client.getShoppingItems();
\end{verbatim}
The above code simply initialises a new variable \textit{request} and assigns it to the request made to the \textit{"/shoppingItems"} endpoints. The spoofed response is assigned the status code 200, imitating a successful operation, and an example of the correct response payload is also injected in. The \textit{getShoppingItems()} can then be called. As far as the methods calling the endpoints are concerned they will have interacted with what they believe to be the live API, but behind the scenes the request will have been intercepted and replaced with the Nock. The consuming code will then treat the faked response the same as a legitimate one and the outputs of the methods can be asserted against to check that they are working correctly.

Failed requests can be built and used in the exact same way. A status code and an error payload just need to be assigned to the Nock request to endpoint and it's good to go.
\begin{verbatim}
    const failedRequest = Nock('http://localhost:3000/REST/1.0')
    .get('/shoppingItems/mango')
    .reply(404, 
        {
            "errorIdentifier": "ShoppingItemNotFound",
            "message": "Shopping Item not found with params: 
                {
                    \"name\":\"mango\"
                }"
        })
    const res = await client.getShoppingItem('mango');
\end{verbatim}
The code above simply creates a Nock imitating a request made for a resource that does not exist. The method that consumes the request is expected to throw an instance of the \textit{ErrorWrapper} object using the data from the error payload and status code. The unit tests for the failed requests will test that the expected object is thrown and that it contains the fields and values that are expected. This is easily done through simple assertions on the object properties within the unit tests.

Once a request has been intercepted and assigned a matching Nock, the Nock is consumed. Any subsequent requests will need to have separate Nock objects assigned to them. This allows the automatic retry facilities of the client to be tested quite easily. The rate limit retry system works by making the first request, checking the status code returned in the case of an error and if supported, a number of retries of the same request will be made with incremental delays between them. Essentially a successful retry system will contain a number of failed requests, followed by a final successful request. With Nock we can define all of these requests and the system should handle them as if they were interacting with an actual rate-limited API. An example for testing a request made successfully after three retries is given below.
\begin{verbatim}
    const failThrice = Nock('http://localhost:3000/REST/1.0')
    .get('/shoppingItems')
    .times(3)
    .reply(429)
    const retriedSuccessRequest = Nock('http://localhost:3000/REST/1.0')
    .get('/shoppingItems') 
    .reply(200,
        {
            page: 1,
            totalPages: 1,
            shoppingItems: [{
                "name":"apple",
                "category":"Fruit",
                "numberOfStock":110,
                "inStock":true
            }]
        })
    const res = await client.getShoppingItems();
\end{verbatim}
There are two Nocks defined above. The first behaves the same as the previous spoofed errors with one exception, it has the additional \textit{.times()} function call attached. All this does it tell Nock that the request should exist three times. This means that when the axios-retry library makes retries the request after the initial failure, it will receive two subsequent rate-limited errors. 

The second Nock is a typical successful mocked response for the same endpoint. When the retry is triggered a third and final time this response will be returned. When the \textit{getShoppingItems()} method is called the output should contain the information provided by the final success. As with the previous tests this can be tested by simple asserting the fields provided in the response body outputted by the method.
\section{End-to-End Testing the Auto-generated Client}
\subsection{Migrating the Unit Tests from my Client}
In order to test the client generated from the OpenAPI Generator I will pull out the unit tests from my handcrafted client and copy them across. The tests will require some very light modification but all of the Nock descriptions and assertions on the responses should be transferable. The only modifications should be changes to the names of methods called and any variance in the method signatures.

The clients package.json file will also need some additional libraries to be imported in order for the unit tests to run, however this will have no bearing on the actual functionality of the client. These will include the Jest framework and Nock library.
\section{Results of the Tests}
In this section I will analyse the Code Coverage reports generated for each of the services to get a general idea of how comprehensively the test suites cover the code-base. The test suite has been designed to focus on the actual business logic of the micro-service and the clients. This is where the core functionality of the systems are located, and some areas of the code do not have specific tests covering them, specifically around failures on database connection or any sort of network connection error. This will lead to some expected gaps in the coverage, which should be shown via line details in the report. If these gaps in coverage are subsequently identified as high risk areas in a quality review, the reports provide sufficient detail to identify the areas for which additional tests should be defined
\subsection{Reading the Code Coverage Reports}
The generated reports contain a detailed breakdown of which files and components of the code are covered, as well as details of the degree to which they are covered. It breaks the coverage down into four key sections, Statements, Branches, Functions and Lines. It also lists the lines in the code which are not covered at all in tests. For the purpose of this report I am going to omit analysis of the Line coverage, as I think the other 3 aspects are more important.
\subsubsection{Statements}
This covers the Statements within the software. Variable assignment, function calls and return values all fall under under this umbrella and the reports details how many of these statements have been executed when the tests have been run. Statement coverage should typically be relatively high as you want the core of your code base to be heavily tested, covering most of the expected use case for the software. However if there are some niche branching paths or function calls that can be difficult to reach with unit tests, it might be unfeasible, or simply not worth the time investment, to strive for full coverage.
\subsubsection{Branches}
The branch coverage shows how many of the branches in the applications control flow have been executed. Typical examples for this are your standard if or switch statements. Maintaining 100\% coverage on control flow can be very difficult, and there are likely to be branches within the code that are not really expected to execute unless in the case of things like server failures. Writing unit tests covering these cases can be time consuming, and as they are not a typical use case of the software, oftentimes are not worth the time investment.
\subsubsection{Functions}
Function coverage should be as close to 100\% as possible and unit tests should be in place for every important function in the application. The function report details how many of the defined functions have actually been executed within the unit tests.
\subsubsection{Lines}
The line coverage tracks how many of the lines within the applications codes have been executed and in the same vein as branch coverage, can be extremely hard to reach perfect coverage. That being said aiming for as high a value as possible within sensible time constraints is best.
\subsection{The Micro-service}
The report below shows the Code Coverage report for the ShoppingItems micro-service.
\begin{figure}[!htb]
\caption{Code Coverage Report for the Exemplar Micro-service}
\centering
\includegraphics[scale=0.62]{FYP_Dissertation_template/Figures/microservice-code-coverage.PNG}
\end{figure}
\FloatBarrier
The files we are particularly interested in tracking are \textit{ShoppingItemAgent}, \textit{ShoppingItemModel}, \textit{RateLimiter} and \textit{ShoppingItemController} as this is where the vast majority of the business and application logic is stored.
\subsubsection{Statements}
Starting with the statement coverage on \textit{ShoppingItemAgent} we can see that the file has 98\% coverage. The report shows the line 40 of the file is uncovered, and upon inspection we can see that it is handling any unexpected errors thrown by the database. This code is relatively unimportant, more acting as a safety net should an unexpected issue occur. Testing this kind of code is relatively unimportant as it has no real bearing on the day to day usage of the application and so the test suite doesn't cover it. In a perfect world test cases would be written for it, but with time constraints both for this project, and that you would also typically see in a consumer facing application the time cost doesn't really justify it. The 98\% coverage exceeds my previously stated desired coverage of 85\%.


\textit{ShoppingItemModel} has 100\% statement coverage as does \textit{RateLimiter}.

\textit{ShoppingItemController} reaches 97\% coverage, with a single uncovered line at line 106. Looking at line 106 in the codebase we can see that it is handling any unexpected errors thrown. As stated before, writing test cases for these events is time consuming and so I opted to move on. Once again, coverage exceeds that of the target. 
\subsubsection{Branches}
The branches follow the exact same pattern as the statement report, with the files without full coverage being impacted by the same lines once again and so I will not go into any more detail on the matter here.

\textit{ShoppingItemModel} \textit{RateLimiter} both had 100\% branch coverage, \textit{ShoppingItemAgent} reached 94\% coverage and \textit{ShoppingItemController} reached 86\% coverage. All files exceeded target.

\subsubsection{Functions}
All of the files being watched maintain 100\% function coverage in the report.
\subsection{The Handcrafted Client}
Figure 6.2 shows the report for the Code Coverage of my custom API client.
\begin{figure}[!htb]
\caption{Code Coverage Report for the Handcrafted Client}
\centering
\includegraphics[scale=0.70]{FYP_Dissertation_template/Figures/handcrafted-client-code-coverage.PNG}
\end{figure}
\FloatBarrier

The key files we will be looking at are \textit{MicroserviceClient} and \textit{Http}. The other files are mainly utility files providing Enums and Type declarations.
\subsubsection{Statements}
Statements coverage of \textit{MicroserviceClient} is 100\% and coverage of \textit{Http} is 95\%. Both reaching the targeted coverage.

If we dive into the uncovered lines within the \textit{Http} file we can see lines 43 and 44 handle any unexpected errors when physically sending the request. Errors under this category would typically be any sort of internet connection issues and as the focus of my testing is on the actual interactions with the API, I have not created any test suites for these cases.
\subsubsection{Branches}
The coverage for branches within \textit{Micro-serviceClient} is 100\% and for \textit{Http} is 81\%.

The \textit{Http} coverage fails to reach the 85\% coverage, but upon inspection we can see that it is only one branch that is not covered, the same code mentioned in the statement report analysis. Covering this case has no real bearing on the success of the project, and so I will opt to leave it be for now.
\subsubsection{Functions}
Both targeted files reached 100\% function coverage within the test suites.
\subsection{The Auto-generated Client with the modifications}
The report for code coverage of the API client generated from the OpenAPIGenerator templates I modified are shown in Figure 6.3. The unit tests were pulled out of my handcrafted client and so with some very minor method signature changes, the test-suite itself is mostly identical across the two clients.
\begin{figure}[!htb]
\caption{Code Coverage Report for the Generated Client with my Modifications}
\centering
\includegraphics[scale=0.55]{FYP_Dissertation_template/Figures/modified-client-code-coverage.PNG}
\end{figure}
\FloatBarrier
I am not going to break down the report for the generated client. There is a lot of functionality within the client I do not care about in the context of this project, and my focus is that the actual test cases I have transferred from my own client pass, as these represent the objectives I will be measuring the success of the generator changes against. As we can see in the figure above, all of the tests passed.
\section{Evaluation against Project Requirements and Specifications}
In this section I will look back at the objectives for the project and determine whether they have been met for each piece of software.
\subsection{The Exemplar Micro-service}
In order to assess how well the framework meets the project objectives, we will now evaluate them against the exemplar micro-service. This is a simple service that could be used by any shopping application and is designed to showcase the combinations of libraries that would be used in any real micro-service.
\subsubsection{The Objectives for the Micro-service}
\begin{enumerate}
    \item \textit{Generate an OpenAPI Specification file}
    \item \textit{Use well-formed URLs}
    \item \textit{Provide endpoints for each of the major HTTP verbs}
    \item \textit{Assign the correct HTTP status codes to both Success and Error Responses}
    \item \textit{Validate both request and response payloads}
    \item \textit{Provide useful documentation for the API}
    \item \textit{Utilise Rate Limiting}
    \item \textit{Make use of Pagination on large responses}
    \item \textit{Provide access to a database to support CRUD usage of different HTTP Verbs}
\end{enumerate}
\subsubsection{Generate an OpenAPI Specification file}
The TSOA library provides the facilities for taking the API definitions defined into the controller files and generates an OpenAPI Specification file from them. It does this through the command:
\begin{verbatim}
    yarn run tsoa spec
\end{verbatim}
The output from this command is either a YAML or JSON OpenAPI Specification.
\subsubsection{Use well-formed URLs}
When designing the ShoppingItem API for my micro-service, the API design best practices stated in Chapter 2 were used as a basis.

Those best practises are as follows:
\begin{enumerate}
    \item Resources should be named with nouns
    \item Resource names should be pluralised
    \item Individual resources should be fetched through use of identifiers
\end{enumerate}
The base URL for the REST API is the following:
\begin{verbatim}
    /REST/1.0/shoppingItems
\end{verbatim}
By looking at the URL we can see that the ShoppingItem resource is named after a noun, and the noun is in it's plural form.

The API endpoint for fetching an individual Shopping item is:
\begin{verbatim}
    /REST/1.0/shoppingItems/{name}
\end{verbatim}
The endpoint is an extension of the base URL, where {name} is the unique name of the targeted individual resource. This name acts as the unique identifier for the resource which in turn checks off the third best practice.
\subsubsection{Provide endpoints for each of the major HTTP verbs}
There are four major HTTP verbs that this objective is referencing: GET, POST, PUT and DELETE. Each of these verbs have associated endpoints within the ShoppingItems API.
\begin{enumerate}
    \item GET
    \begin{verbatim}
        /REST/1.O/shoppingItems
        /REST/1.0/shoppingItems/{name}
    \end{verbatim}
    \item POST
    \begin{verbatim}
        /REST/1.0/shoppingItems
    \end{verbatim}
    \item PUT
    \begin{verbatim}
        /REST/1.0/shoppingItems/{name}/category
        /REST/1.0/shoppingItems/{name}/increaseStock
        /REST/1.0/shoppingItems/{name}/decreaseStock
    \end{verbatim}
    \item DELETE
    \begin{verbatim}
        /REST/1.0/shoppingItems/{name}
    \end{verbatim}
\end{enumerate}
\subsubsection{Assign the correct HTTP status codes to both Success and Error Responses}
Correctly assigning HTTP status codes are crucial if consumers are going to successfully use the REST API. The micro-service API follows the standardised assignment of status codes explored in Chapter 2.

Success responses for any request that returns a resource  are assigned a 200 status code alongside the response body. For the creation of a new resource the status code 201 is assigned and a 204 status code is used when a resource is deleted.

When a resource is not found a status code of 404 is used, and in the case of a bad request 400 is used. Any unexpected error on the server side is assigned a 500 status code. A request that breaks the rate limits of the API is assigned a 429 status code.
\subsubsection{Validate both request and response payloads}
The TSOA library is used for creating all of the routing and API generation for the micro-service. Out of the box TSOA handles all of the validation for the API endpoints, using TypeScript types to validate both incoming and outgoing payloads and blocking any that do match the expected format.

When an endpoint is defined that takes in a request body, such as the endpoint associated with the creation of a new ShoppingItem, the body is cast as a TypeScript type. When a request hits the API endpoint TSOA takes the body from the HTTP message and compares it to the assigned type. If there are missing required fields, has unexpected fields or the body is of an unexpected format then it will throw a ValidationError with a 400 status code to the consumer.

Similarly the response body is defined by a TypeScript type and assigned to the return type of the controller method associated with the endpoint. As TypeScript is a strongly-typed language this tightly weaves the response into the lower level code handling the business logic of the request. If the method that the controller calls returns a different type then TypeScript itself will thrown an error on compile time.
\subsubsection{Provide useful documentation for the API}
The micro-service has the ability to generate an OpenAPI Specification representing a full definition of it's API. The swagger-ui-express library takes this document and parses it into a HTML page fully detailing the endpoints using the information provided in the specification. An API endpoint that serves this page can be assigned to the server, allowing the creation of a full documentation endpoint that any user can hit to get the details of the API.

Whilst this library generates the actual page, it works in tandem with TSOA. When TSOA generates the OpenAPI specification it scours the types and methods used for both additional decorators and JSDoc notation. These optional additions to the files allows developers to add a large volume of extra information and descriptions to various fields, methods and payloads. This extra data is then injected into specification file. By documenting the code itself, developers can automatically generate fully descriptive API documentation.
\begin{figure}[!htb]
\caption{Documentation snippet from the micro-service documentation URL}
\centering
\includegraphics[scale=0.41]{FYP_Dissertation_template/Figures/doc-example.PNG}
\end{figure}
\FloatBarrier
Figure 6.4 above shows part of this HTML page. Information relating to each of the query parameters, an overall endpoint description and example response payload are provided.

Outside of acting as documentation, the swagger-ui-express library also allows requests to be made directly through the web-page, saving time writing complex and tedious CURL commands when testing the API.
\subsubsection{Utilise Rate Limiting}
The express-rate-limit library is used to add rate-limiting functionality to the micro-service. Whilst it is a relatively simple integrations, applying a global rate limit to the service rather than a per user limit, it works fine as a proof of concept. Outside of intercepting requests that would break the limit and throwing a 429 error, it also inject a number of rate limit meta-data headers into the responses of the API.
\begin{figure}[!htb]
    \caption{Example of Rate Limit response headers provided by the API}
\centering
\includegraphics[scale=0.6]{FYP_Dissertation_template/Figures/rate-limit.PNG}
\end{figure}
\FloatBarrier
The above figure shows the response headers include information about the rate limit itself, number of remaining requests before the limit is exceeded and a timestamp for when the current limit expires. This information can be used by the API consumer in order to request any failed requests, or plan how to make a series of requests to the API.
\subsubsection{Make use of Pagination on large responses}
The micro-service breaks down requests returning multiple resources into chunks called pages. Through usage of the mongoose-paginate-v2 library this pagination is done directly at the database request level. When the data-agent queries the database, the library wraps the request and returns a subset of the result based on optional parameters passed into the method signature. It also provides a number of meta-data fields providing additional information about the pagination including total number of pages and the current page number.

By handling the pagination at the data-agent layer, we do not have to do any additional processing on the request in order to pull out the pages resulting in simpler and more accessible code. It also allows a number of query parameters to be provided at the controller level which can simply be plugged into the optional paging parameters in the data-agents methods signature. This allows the API consumer to cycle through the pages by passing in the desired information into the query parameters, with default options selected if none of these parameters are provided.

A paginated response example is shown in Figure 6.6 below.
\begin{figure}[!htb]
    \caption{Example of Paginated Response provided by the API}
\centering
\includegraphics[scale=0.4]{FYP_Dissertation_template/Figures/pagination-example.PNG}
\end{figure}
\FloatBarrier
\subsubsection{Provide access to a database to support CRUD usage of different HTTP Verbs}
The micro-service connects to a MongoDB database which stores all of the services resource information. When the createShoppingItem() method is called a new document is created in the ShoppingItems collection and operations fetching, modifying and deleting the resource(s) can be triggered through the API and data-agent.
\subsection{The Handcrafted Client}
By evaluating my handcrafted client against its objectives, I can determine whether it successfully fulfilled its function.
\subsubsection{The Objectives for the Handcrafted Client}
\begin{enumerate}
    \item \textit{Support all endpoints across the API}
    \item \textit{Provide details of the responses types returned by endpoints}
    \item \textit{Provide details of all input parameters for the endpoints}
    \item \textit{Ensure all important error information is correctly is passed on}
    \item \textit{Automatically retry requests that fail with certain HTTP status codes}
\end{enumerate}
\subsubsection{Support all endpoints across the API}
The client contains methods for calling each of the endpoints exposed by the API, with the exception of the endpoint:
\begin{verbatim}
    GET /REST/1.0/documentation
\end{verbatim}

An API consumer would never have to call this endpoint during an integration, and it would instead be used by the API server to create a public facing documentation page.

The methods and their corresponding endpoints are detailed in the table below.

\begin{table}[hbt!]
\centering
\caption{Method name to URL mapping for the Handcrafted Client}
\label{tab:handcrafted-client-ref-table}
\begin{tabular}{ll}
\hline
\multicolumn{1}{|l|}{\textbf{Method Name}}                  & \multicolumn{1}{l|}{\textbf{URL Endpoint}}\\ \hline
\multicolumn{1}{|l|}{getShoppingItems()}           & \multicolumn{1}{l|}{GET /REST/1.0/shoppingItems}                        \\ \hline
\multicolumn{1}{|l|}{getShoppingItem()}            & \multicolumn{1}{l|}{GET /REST/1.0/shoppingItems/\{name\}}               \\ \hline
\multicolumn{1}{|l|}{createShoppingItem()}         & \multicolumn{1}{l|}{POST /REST/1.0/shoppingItems}                       \\ \hline
\multicolumn{1}{|l|}{deleteShoppingItem()}         & \multicolumn{1}{l|}{DELETE /REST/1.0/shoppingItems/\{name\}}            \\ \hline
\multicolumn{1}{|l|}{updateShoppingItemCategory()} & \multicolumn{1}{l|}{PUT /REST/1.0/shoppingItems/\{name\}/category}      \\ \hline
\multicolumn{1}{|l|}{increaseShoppingItemStock()}  & \multicolumn{1}{l|}{PUT /REST/1.0/shoppingItems/\{name\}/increaseStock} \\ \hline
\multicolumn{1}{|l|}{decreaseShoppingItemStock()}  & \multicolumn{1}{l|}{PUT /REST/1.0/shoppingItems/\{name\}/decreaseStock} \\ \hline               
\end{tabular}
\end{table}
\FloatBarrier

\subsubsection{Provide details of the responses types returned by endpoints}
The handcrafted client is built using TypeScript which allows us to build up Types for each of the response bodies returned by the API. The Axios library  uses generics to provide details of the expected response type of the request. By casting the return types of each method to the custom response types I defined, Axios will inject this typing into the data object that it returns.

Figure 6.7 shows an example of this casting.
\begin{figure}[!htb]
    \caption{Example Response Type Casting}
\centering
\includegraphics[scale=0.6]{FYP_Dissertation_template/Figures/handcrafted-client-response-type-casting.PNG}
\end{figure}
\FloatBarrier

Consumers of the method can then access all of the fields from the response when the data field is pulled out of the methods output.

Figure 6.8 shows the usage of this data field.
\begin{figure}[!htb]
    \caption{Example Usage of the Response Typings}
\centering
\includegraphics[scale=0.6]{FYP_Dissertation_template/Figures/handcrafted-client-response-type-return.PNG}
\end{figure}
\FloatBarrier
\subsubsection{Provide details of all input parameters for the endpoints}
As with detailing the shapes of the responses, TypeScript allows type casting for each of the input parameters for methods. These input parameters each represent the various path, query and request body parameters for the API endpoints, and proper type definitions within the method signatures allow full details of these parameters to be exposed to the user.

Figure 6.9 below shows how the client exposes the typings of these fields through the method signature.
\begin{figure}[!htb]
    \caption{Example Usage of the Request Typings}
\centering
\includegraphics[scale=0.57]{FYP_Dissertation_template/Figures/handcrafted-client-request-typings.PNG}
\end{figure}
\FloatBarrier
\subsubsection{Ensure all important error information is correctly is passed on}
The handcrafted client makes use of a custom Axios interceptor to catch and handle any errors thrown during the requests. The fields are pulled out of the error payload and are used to create a local instance of the same ErrorWrapper that the the micro-service itself throws. This ensures that both the server and client in the request cycle are using the same error format when a request fails.

The unit test suite contains a number of test case that explicitly check that these errors are handled as expected.

Figure 6.10 shows a passing unit test that checks the error thrown on a request that is expected to fail.
\begin{figure}[!htb]
    \caption{Example Unit Test for Error Handling}
\centering
\includegraphics[scale=0.57]{FYP_Dissertation_template/Figures/handcrafted-client-error-handling.PNG}
\end{figure}
\FloatBarrier
\subsubsection{Automatically retry requests that fail with certain HTTP status codes}
The client makes use of the \textit{axios-retry} library to retry failed requests. The library injects an Axios interceptor that checks the HTTP status code of the failed response, and if it falls under the subset of codes that should be retried, it will re-submit the request. Each request can be retried a total of 3 times before the request is aborted and an error message is bubbled back up to the user.

The test suite contains a set of tests that validate this behaviour. These tests cover both successful and failed retry attempts and give enough coverage to give confidence in the feature. The key focus for these tests is retrying rate limited requests.

The unit tests covering retrying rate limited requests are shown below in Figure 6.11.
\begin{figure}[!htb]
    \caption{Example Unit Test for Retrying Rate Limited Requests}
\centering
\includegraphics[scale=0.45]{FYP_Dissertation_template/Figures/rate-limited-handcrafted.PNG}
\end{figure}
\FloatBarrier
\subsection{The Modified Auto-generated Client}
The final component of the project was taking the default Typescript-Axios template for the OpenAPIGenerator and modifying it to produce a generated client that will satisfy the same objectives as my home-brewed client. Evaluation of the objectives against a client generated for the micro-service API are detailed in the following section, however a client generated for any API would workt.
\subsubsection{The Objectives for the Auto-generated Client}
\begin{enumerate}
    \item \textit{Support all endpoints across the API}
    \item \textit{Provide details of the responses types returned by endpoints}
    \item \textit{Provide details of all input parameters for the endpoints}
    \item \textit{Ensure all important error information is correctly is passed on}
    \item \textit{Automatically retry requests that fail with certain HTTP status codes}
\end{enumerate}
\subsubsection{Support all endpoints across the API}
The OpenAPIGenerator uses a number of Mustache template files to define the shape of the generated code. Information is pulled out of the OpenAPI Specification for the API and used to populate parameters within these template files. These populated templates are then used by a templating engine to create the files containing the code. As the specification generated by the micro-service contains definitions for each of the API endpoints, the generated client will in turn contain methods handling all of them too. 

The only endpoint missing from the specification, and subsequently the client, is the endpoint serving the API documentation. This however is by design as this endpoint is not needed for operation of the API itself.

The table below shows a mapping of the various API endpoints to the associated client methods.

\begin{table}[hbt!]
\centering
\caption{Method name to URL mapping for the Generated Client}
\label{tab:handcrafted-client-ref-table}
\begin{tabular}{ll}
\hline
\multicolumn{1}{|l|}{\textbf{Method Name}}                  & \multicolumn{1}{l|}{\textbf{URL Endpoint}}\\ \hline
\multicolumn{1}{|l|}{getShoppingItems()}           & \multicolumn{1}{l|}{GET /REST/1.0/shoppingItems}                        \\ \hline
\multicolumn{1}{|l|}{getShoppingItem()}            & \multicolumn{1}{l|}{GET /REST/1.0/shoppingItems/\{name\}}               \\ \hline
\multicolumn{1}{|l|}{createShoppingItem()}         & \multicolumn{1}{l|}{POST /REST/1.0/shoppingItems}                       \\ \hline
\multicolumn{1}{|l|}{deleteShoppingItem()}         & \multicolumn{1}{l|}{DELETE /REST/1.0/shoppingItems/\{name\}}            \\ \hline
\multicolumn{1}{|l|}{updateShoppingItemCategory()} & \multicolumn{1}{l|}{PUT /REST/1.0/shoppingItems/\{name\}/category}      \\ \hline
\multicolumn{1}{|l|}{increaseShoppingItemStock()}  & \multicolumn{1}{l|}{PUT /REST/1.0/shoppingItems/\{name\}/increaseStock} \\ \hline
\multicolumn{1}{|l|}{decreaseShoppingItemStock()}  & \multicolumn{1}{l|}{PUT /REST/1.0/shoppingItems/\{name\}/decreaseStock} \\ \hline               
\end{tabular}
\end{table}
\FloatBarrier
\subsubsection{Provide details of the responses types returned by endpoints}
The templates used for client generation are also based on the Axios library. As with my client, the generated client injects types into the generic Axios method calls to define the shape of the response bodies returned when the requests are made. 

There are however some slight differences between the two clients. Where my custom types were statically defined, the types within the generated are built using information provided from the specification file themselves. Functionally this produces the same result, with the library providing details of the response bodies for each request through the data object returned by Axios.
\subsubsection{Provide details of all input parameters for the endpoints}
As with the response details, the Typescript code generated uses information from the specification to create strongly typed methods for each of the API endpoints. 

Each method contains typed method parameters representing each of the URL parameters, query parameters or request body. Individual URL or query parameters are assigned a TypeScript type best matching the type in the specification. In the case of fields that can only match specific values, enumeration types are generated. Any JSON payload from the specification generates a TypeScript interface that the field is then assigned.
\subsubsection{Ensure all important error information is correctly is passed on}
Both the custom error wrapper and the error handler middleware are pulled out of my handcrafted client, and injected in an identical way into the templates for the OpenAPI Generator. This results in the behaviour being identical across the two clients, with the test-suite covering cases validating this.
\subsubsection{Automatically retry requests that fail with certain HTTP status codes}
As with the error handling, the mechanism for retrying failed requests is shared across both the handcrafted client, and the client template for the generator.  Functionally the two clients handle the retries identically, and both services contain unit-tests covering this functionality.